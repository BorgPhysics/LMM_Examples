from collections import Counter
import numpy as np
import os
import pandas as pd
import pprint
import torch
from torch import Tensor
from sentence_transformers import util

from ..data import data_loader
from ..directives import directives_processor
from ..models import model_settings
from ..faiss_index import faiss_index_processes as index_processor

dp = directives_processor.DirectivesProcessor()
try:
    index_processor.load_index()
except Exception as e:
    print('Index failed to load.  Attempting to rebuild it...')
    df = data_loader.get_document_index()
    index_transformer_model, _  = model_settings.get_index_transformer_model_and_tokenizer()
    index_processor.load_index(df, index_transformer_model)    

index = index_processor.index
sentence_to_index_mapping = index_processor.sentence_to_index_mapping

question_and_answer_history = []
history_record = None

# Unfortunately, I'll need a bunch of global settings for now.
qa_num_return_sequences=2

def run_preprocessing_commands(preprocessing_commands):
    global qa_num_return_sequences
    # Loop through the pretraining commands to get the things that we need
    for preprocessing_command in preprocessing_commands:
        qa_gen_count = preprocessing_command.get('number_of_qa_sentences_to_generate', None)
        if qa_gen_count:
            qa_num_return_sequences = qa_gen_count
        if preprocessing_command.get('check_for_new_docs', False):            
#             df = data_loader.get_document_index()
#             index_transformer_model, _  = model_settings.get_index_transformer_model_and_tokenizer()
#             index_processor.load_index(df, index_transformer_model)
#             index = index_processor.index
#             sentence_to_index_mapping = index_processor.sentence_to_index_mapping            
            
            data_loader.update_document_index()
            index_processor.load_index()
            index = index_processor.index
            sentence_to_index_mapping = index_processor.sentence_to_index_mapping

def run_lmm_tests(hist_record=None):
    global history_record
    history_record = hist_record
    
    # TASKS:
    # Generate question, query index, generate prompts for final LMM, get final answer
    # The Ensembles example has this in the code: if not pretrained_model_directory:
    # This can be substituted with questions and answers that have occurred in this 'conversation'
    print('Running LMM Tests...')
    questions = get_questions()
    for question in questions:
        # For each question, get the list of best sentence fragment matches from the index.
        most_similar_prompts = get_index_responses(question)
    
        # Now that you have the most_similar_prompts, call the method to run the query
#         answer = answer_query(question_and_answer_history, question, most_similar_prompts)
        answer = answer_query_using_all_relevant_docs(question_and_answer_history, question, most_similar_prompts)

def answer_query(question_and_answer_history, question, most_similar_prompts):
    ##################################################
    ##### EXTRACT SENTENCES AND ORIGINATING DOCS #####
    ##################################################
    # Extract only the sentences from most_similar_prompts
    faiss_sentences = [sentence for _, _, sentence in most_similar_prompts]    
    # Temporary until I find a better way to question all of the relevant documents
    # Extract doc_ids from most_similar_prompts
    doc_ids = [doc_id for doc_id, _, _ in most_similar_prompts]
    unique_doc_ids = set(doc_ids)
    print('doc_ids', doc_ids)
    print('unique_doc_ids', unique_doc_ids)
    # Count occurrences of each doc_id
    doc_id_counts = Counter(doc_ids)
    # Find the doc_id with the highest count
    most_common_doc_id, occurrences = doc_id_counts.most_common(1)[0]
    ##################################################
    ##### EXTRACT SENTENCES AND ORIGINATING DOCS #####
    ##################################################
    
    doc_result = data_loader.get_info_by_doc_id(most_common_doc_id) 
    answer = 'FAILED'
    if doc_result is not None:
#         doc_id = doc_result['doc_id']
        doc_filename = doc_result['doc_filename']
        document = doc_result['doc_contents'] 
        print('most_common_doc_id:', most_common_doc_id, doc_filename)  
        answer = process_single_large_document(question_and_answer_history, question, faiss_sentences, document)
        print('\n\nUser question:', question, '\nAnswer:', answer)
        
    return answer

def answer_query_using_all_relevant_docs(question_and_answer_history, question, most_similar_prompts):
    ##################################################
    ##### EXTRACT SENTENCES AND ORIGINATING DOCS #####
    ##################################################
    # Extract only the sentences from most_similar_prompts
    faiss_sentences = [sentence for _, _, sentence in most_similar_prompts]    
    # Temporary until I find a better way to question all of the relevant documents
    # Extract doc_ids from most_similar_prompts
    doc_ids = [doc_id for doc_id, _, _ in most_similar_prompts]
    unique_doc_ids = set(doc_ids)
    print('doc_ids', doc_ids)
    print('unique_doc_ids', unique_doc_ids)
    # Count occurrences of each doc_id
    doc_id_counts = Counter(doc_ids)
    # Find the doc_id with the highest count
    most_common_doc_id, occurrences = doc_id_counts.most_common(1)[0]
    ##################################################
    ##### EXTRACT SENTENCES AND ORIGINATING DOCS #####
    ##################################################
    generated_summaries = []
    for doc_id in unique_doc_ids:
        doc_result = data_loader.get_info_by_doc_id(doc_id) 
        if doc_result is not None:
            doc_filename = doc_result['doc_filename']
            document = doc_result['doc_contents'] 
            print('\n\nUser question:', question)
            print('PROCESSING doc_id:', most_common_doc_id, doc_filename)
            summaries = process_single_large_document(question_and_answer_history, question, faiss_sentences, document)
            generated_summaries.extend(summaries)            
    
    # TODO: Walk through the tensors in the generated sequences, check their similarity and generate a final summary of all of them.
    print('Processing', len(generated_summaries), 'summaries.')
#     print('type(generated_summaries)', type(generated_summaries), '\n', generated_summaries)
    answer = get_answer_from_summaries(question, faiss_sentences, generated_summaries)
    
    print('\n\nUser question:', question, '\nFINAL ANSWER from all summaries:', answer)
    return answer

def get_answer_from_summaries(question, faiss_sentences, generated_summaries):
    global history_record
    index_transformer_model, index_transformer_tokenizer = model_settings.get_index_transformer_model_and_tokenizer()
    
    # TODO: Compare their similarities, log the metrics,generate a final summary from all of them and log that also.
    # Metrics should include cosine similarity between all pairs, a final, overall cos-sim average and the best cos-sim.
    print('Generating a final answer and statistics')
    merged_summaries = ''
    spacer = ''
    count = 0
    for generated_summary in generated_summaries:
        count += 1
#         print('Merging generated_sequence type:', type(generated_summary), '\n', generated_summary)
        merged_summaries += spacer + generated_summary
        spacer = ' '
        
    final_summary = model_settings.summarize_text(merged_summaries, max_length=500)
    # Compare the final summary to each of the generated summaries
    best_summary_to_final_score = 0.0
    average_summary_comparison_score = 0.0    
    
    score_total = 0.0
    question_embedding = index_transformer_model.encode(question)
    final_embedding = index_transformer_model.encode(final_summary)
    question_to_final_answer_score = util.cos_sim(question_embedding, final_embedding).item()
    
    for generated_summary in generated_summaries:
        generated_summary_embedding = index_transformer_model.encode(generated_summary)
        cos_sim = util.cos_sim(generated_summary_embedding, final_embedding).item()
        score_total += cos_sim
        if history_record:
            history_record.log_metric("gen_to_final", cos_sim)
        if cos_sim > best_summary_to_final_score:
            best_summary_to_final_score = cos_sim
    
    average_summary_comparison_score = score_total / count
    print('The average summary comparison score is', average_summary_comparison_score, 
          '\n\tThe best summary comparison score is', best_summary_to_final_score, 
          '\n\tThe final summary comparison to the question is', question_to_final_answer_score)
    if history_record:
        history_record.log_metric("best_to_final", best_summary_to_final_score)
    
    return final_summary
    
def process_single_large_document(question_and_answer_history, user_question, faiss_sentences, large_document):
    global qa_num_return_sequences
    qa_model, qa_tokenizer = model_settings.get_qa_model_and_tokenizer()

    # Concatenate FAISS sentences with the user question to form the context
    faiss_context = ' '.join(faiss_sentences)
    context = user_question + " " + faiss_context

    # Tokenize the context
    inputs = qa_tokenizer.encode(context, return_tensors="pt")

    # Set the maximum sequence length according to the model's maximum token limit
    max_length = qa_model.config.max_position_embeddings

    # Print the length of the combined context and max length
    print("Length of combined context:", len(context.split()), 'max_length:', max_length)

    # Generate text using GPT-2 model
    generated = qa_model.generate(inputs, max_length=max_length, num_return_sequences=qa_num_return_sequences, do_sample=True)
    
    # Decode and print the generated text
    summaries = []
    for i, g in enumerate(generated):
        decoded_text = qa_tokenizer.decode(g, skip_special_tokens=True)        
        # Remove the user question from the decoded text
        if user_question in decoded_text:
            decoded_text = decoded_text.replace(user_question, "").strip()
            
        print(f"\nGENERATED SEQUENCE {i + 1}: {decoded_text[:300]}")
        # Summarize the generated text
        summarized_text = model_settings.summarize_text(decoded_text, max_length=500)
        print(f"\nSUMMARIZED SEQUENCE {i + 1}: {summarized_text}\n")
        summaries.append(summarized_text)

#     return generated  
    return summaries

def get_questions():
    questions = ["Could you provide a comprehensive overview of the impact of climate change on global health, considering various aspects and potential implications?"]
    questions = ["How is global health affected by climate change?"]
    return questions

def get_index_responses(user_query):
    similarity_threshold = 0.25
    index_transformer_model, _  = model_settings.get_index_transformer_model_and_tokenizer()
    # Encode user query into an embedding
    user_query_embedding = index_transformer_model.encode(user_query, convert_to_tensor=True).numpy()
    
    # Search in FAISS index
    k = 25  # Number of most similar prompts to retrieve
    D, I = index.search(np.array([user_query_embedding]), k)
    
    
    # Filter responses based on similarity threshold
    most_similar_prompts = []
    filtered_responses = []
    for distance, idx_num in zip(D[0], I[0]):
        similarity_score = 1 - distance  # Calculate similarity score
        if similarity_score >= similarity_threshold:
            idx = str(idx_num)
            if idx in sentence_to_index_mapping:
                mapping = sentence_to_index_mapping[idx]
                doc_idx = mapping['document_index']
                sentence_idx = mapping['sentence_index']
                sentence = mapping['sentence_text']
                print('Sim score is', similarity_score, 'for:', sentence)
                most_similar_prompts.append((doc_idx, sentence_idx, sentence))
            
    print('\nThere were', len(most_similar_prompts), 'sentences found out of a possible', k, 'that were above the threshold of', similarity_threshold)
    return most_similar_prompts
    

def run_various_components():
    df = data_loader.get_document_index()
     
#     # Generate a list of 'questions' from the document directives
#     topic_names = dp.get_topic_names()
#     print('TOPIC NAMES:', topic_names)
#     topic_titles = dp.get_topic_titles()
#     print('\nTOPIC TITLES:\n', topic_titles)
#     print('\nAI DIRECTIVES:\n', dp.get_topic_text('AI'))
    
    index_transformer_model, _  = model_settings.get_index_transformer_model_and_tokenizer()
#     print('index_transformer_model:\n\t', index_transformer_model)
#     summarization_model, summarization_tokenizer = model_settings.get_summarization_model_and_tokenizer()
#     print('summarization_model:\n\t', summarization_model)
#     qa_model, qa_tokenizer = model_settings.get_qa_model_and_tokenizer()
#     print('qa_model:\n\t', qa_model) 
    
    
    index_processor.load_index(df, index_transformer_model)
    index = index_processor.index
    sentence_to_index_mapping = index_processor.sentence_to_index_mapping
    
#     print('sentence_to_index_mapping:\n\t')
#     pp = pprint.PrettyPrinter(indent=4)
# #     pp.pprint(sentence_to_index_mapping)
    
#     # OR - Check the first 10 entries
#     # Slice the first 5 items of the dictionary
#     first_5_items = {k: sentence_to_index_mapping[k] for k in sorted(sentence_to_index_mapping)[:5]}
#     pp.pprint(first_5_items)
    
#     mapping = sentence_to_index_mapping.get("790")
#     print('mapping:', mapping)
    
    test_user_query(df, index_transformer_model, index, sentence_to_index_mapping)
    
    
#     print('The document index contains', len(df), 'records.\n\t', df.head(1))
    
#     doc_id = 'e5e9975d-5826-4664-83a7-92115931b302'
#     result = data_loader.get_info_by_doc_id(doc_id)    
#     if result is not None:
#         doc_id = result['doc_id']
#         doc_filename = result['doc_filename']
#         doc_contents = result['doc_contents']
# #         print(f"Document ID: {doc_id}")
# #         print(f"Document Filename: {doc_filename}")
# #         print(f"Document Contents: {doc_contents[:150]}")
#         summary = model_settings.summarize_text(doc_contents)
#         print(doc_id, 'summary:\n\t', summary)
#     else:
#         print("Document ID", doc_id, "not found in the DataFrame")
    
#     # Summarize each document using DataFrame
#     for index, row in df.iterrows():
#         doc_id = row['doc_id']
#         doc_filename = row['doc_filename']
#         doc_contents = row['doc_contents']

#         print(f"Document ID: {doc_id}, File Name: {doc_filename}")
#         print('******************************')

#         # Get the content from the DataFrame and summarize it
#         summary = model_settings.summarize_text(doc_contents)
#         print('Summary:\n\t', summary)
#         print('******************************\n')
    
def test_user_query(df, model, index, sentence_to_index_mapping):
    # Run a user query against the index and get the top 5
    # User query or question
    user_query = "How does AI affect global health?"
    user_query = "How is global health affected by climate change?"

    # Encode user query into an embedding
    user_query_embedding = model.encode(user_query, convert_to_tensor=True).numpy()

    # Search in FAISS index
    k = 5  # Number of most similar prompts to retrieve
    D, I = index.search(np.array([user_query_embedding]), k)
    print(D, I)

    # Retrieve sentences and corresponding documents based on valid indices
    most_similar_prompts = []
    for idx in I[0]:
        print('Searching for', idx, 'in sentence_to_index_mapping.')
        mapping = sentence_to_index_mapping.get(str(idx))
        doc_idx = mapping['document_index']
        sentence_idx = mapping['sentence_index']
        sentence = mapping['sentence_text']
        most_similar_prompts.append((doc_idx, sentence_idx, sentence))

    print('\nUSER QUERY:', user_query)
    print("Most similar prompts to the user query:")
#     print('\t', most_similar_prompts)
    for doc_id, sentence_idx, sentence in most_similar_prompts:
        result = data_loader.get_info_by_doc_id(doc_id) 
        if result is not None:
            doc_id = result['doc_id']
            filename = result['doc_filename']
            print(f"\nDocument Index: {doc_idx}\t Sentence Index: {sentence_idx}\t Filename: {filename}")
            print(f"Sentence:\n******************************\n\t{sentence}")
            print('******************************\n')
        else:
            print("Document ID", doc_id, "associated with sentence_idx", sentence_idx, "not found in the DataFrame")
    