{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397dec6e",
   "metadata": {},
   "source": [
    "## Question Answering Pipeline Tests using ML-CHAMP\n",
    "\n",
    "Now that we have an initial working QA pipeline, we can begin to fine-tune the various components using ML-CHAMP to test and document various changes and settings while documenting the code that was generated, FAISS indexes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1824fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "from ml_champ import Project\n",
    "from ml_champ import Ensemble\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f5de3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tworking_directory: D:\\JupyterPrograms\\0-CHAT_GPT\\EXPERIMENTS\\ML_CHAMP\\QA_Pipeline\\QA_Pipeline_Testing\n",
      "Importing modules...\n"
     ]
    }
   ],
   "source": [
    "project_name = 'QA_Pipeline_Testing'\n",
    "project = Project(project_name)\n",
    "# print(dir(project))\n",
    "# print(dir(project.model))\n",
    "\n",
    "summary_model, summary_tokenizer = None, None\n",
    "question_answer_model, question_answer_tokenizer = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4421d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload_models_as_needed():\n",
    "    global project\n",
    "    global summary_model\n",
    "    global summary_tokenizer\n",
    "    global question_answer_model\n",
    "    global question_answer_tokenizer\n",
    "    \n",
    "    ai_project = project.model.get_ai_project()\n",
    "    from ai_project.qa_pipeline.models import model_settings\n",
    "\n",
    "    # from model_settings import get_summarization_model_and_tokenizer\n",
    "    if summary_model and summary_tokenizer:\n",
    "        if not model_settings.summarization_model and not model_settings.summarization_tokenizer:\n",
    "            model_settings.set_summarization_model_and_tokenizer(summary_model, summary_tokenizer)\n",
    "\n",
    "    # from model_settings import get_qa_model_and_tokenizer\n",
    "    if question_answer_model and question_answer_tokenizer:\n",
    "        if not model_settings.qa_model and not model_settings.qa_tokenizer:\n",
    "            model_settings.set_qa_model_and_tokenizer(question_answer_model, question_answer_tokenizer)\n",
    "    \n",
    "def set_models_as_needed():\n",
    "    global project\n",
    "    global summary_model\n",
    "    global summary_tokenizer\n",
    "    global question_answer_model\n",
    "    global question_answer_tokenizer\n",
    "    \n",
    "    ai_project = project.model.get_ai_project()\n",
    "    from ai_project.qa_pipeline.models import model_settings\n",
    "    \n",
    "    if not summary_model or not summary_tokenizer:\n",
    "        summary_model, summary_tokenizer = model_settings.get_summarization_model_and_tokenizer()\n",
    "        \n",
    "    if not question_answer_model or not question_answer_tokenizer:\n",
    "        question_answer_model, question_answer_tokenizer = model_settings.get_qa_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f755ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tworking_directory: D:\\JupyterPrograms\\0-CHAT_GPT\\EXPERIMENTS\\ML_CHAMP\\QA_Pipeline\\QA_Pipeline_Testing\n",
      "Importing modules...\n",
      "CLEARED previous modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\transformers_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called AI_Project.AI_Model.run_preprocessing()...\n",
      "CSV file already exists at: D:/JupyterPrograms/0-CHAT_GPT/EXPERIMENTS/ML_CHAMP/data/document_guid_lookup.csv\n",
      "Loaded existing DataFrame of size 12\n",
      "Loaded sentence_to_index_mapping from D:/JupyterPrograms/0-CHAT_GPT/EXPERIMENTS/ML_CHAMP/data/sentence_to_index_mapping.json\n",
      "Running LMM Tests...\n",
      "Sim score is 0.5013275444507599 for: It remains unclear what the full impact of AI technology on national security\n",
      "will be, and how fast it will arrive.\n",
      "\n",
      "There were 1 sentences found out of a possible 50 that were above the threshold of 0.5\n",
      "unique_doc_ids {'61f06291-5385-4dfb-acda-88f74e955c15'}\n",
      "\n",
      "PROCESSING doc_id: 61f06291-5385-4dfb-acda-88f74e955c15 AI_2.txt\n",
      "\n",
      "User question: What are the main concerns with AI with respect to terrorism?\n",
      "LOADING the QA model and tokenizer...\n",
      "Length of combined context: 32 max_length: 1024\n",
      "\n",
      "GENERATED SEQUENCE 1: It remains unclear what the full impact of AI technology on national security\n",
      "will be, and how fast it will arrive. There are many problems. First, it is hard to predict. People will start to say \"it's too early to say what the fallout will be\" and that might lead to other problems to emerge, such a\n",
      "LOADING the summarization model and tokenizer...\n",
      "\n",
      "SUMMARIZED SEQUENCE 1: It remains unclear what the full impact of AI technology on national security will be, and how fast it will arrive. The U.S. needs a policy in place where we will support any government that wants to use this technology.\n",
      "\n",
      "Processing 1 summaries.\n",
      "Generating a final answer and statistics\n",
      "\tThe average summary comparison score is 0.9939002990722656 \n",
      "\tThe best summary comparison score is 0.9939002990722656 \n",
      "\tThe final summary comparison to the question is 0.7442963123321533\n",
      "\n",
      "User question: What are the main concerns with AI with respect to terrorism? \n",
      "FINAL ANSWER from all summaries:\n",
      " It remains unclear what the full impact of AI technology on national security will be. The U.S. needs a policy in place where we will support any government that wants to use this technology.\n",
      "USING THE PRELOADED summarization model and tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 6):\n",
    "#     print(i)\n",
    "initial_question = None\n",
    "initial_question = \"What are the main concerns with AI with respect to terrorism?\"\n",
    "    \n",
    "project = Project(project_name)\n",
    "preload_models_as_needed()\n",
    "    \n",
    "preprocessing_command = {}\n",
    "\n",
    "if initial_question:\n",
    "    preprocessing_command = {'initial_question': initial_question}\n",
    "    project.model.run_preprocessing(preprocessing_command)    \n",
    "\n",
    "# preprocessing_command = {\"similarity_threshold\": 0.4}\n",
    "# project.model.run_preprocessing(preprocessing_command)\n",
    "# preprocessing_command = {\"number_of_qa_sentences_to_generate\": 1}\n",
    "# project.model.run_preprocessing(preprocessing_command)\n",
    "# preprocessing_command = {\"check_for_new_docs\": True}\n",
    "# project.model.run_preprocessing(preprocessing_command)\n",
    "\n",
    "# Set the command and train from the previous run\n",
    "# project.model.train_model()\n",
    "set_models_as_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5154a217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a question (type 'STOP' to end): STOP\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"Please enter a question (type 'STOP' to end): \")\n",
    "    \n",
    "    if user_input.upper() == 'STOP':\n",
    "        break\n",
    "    \n",
    "    postprocessing_command = {\"print\": user_input}\n",
    "    project.model.run_postprocessing(postprocessing_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4abe3d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for running WINDOWS server on port 5002 using command: netstat -aon | findstr \":5002 \"\n",
      "project_directory: D:\\JupyterPrograms\\0-CHAT_GPT\\EXPERIMENTS\\ML_CHAMP\\QA_Pipeline\\QA_Pipeline_Testing\n",
      "Checking for running WINDOWS server on port 5002 using command: netstat -aon | findstr \":5002 \"\n",
      "Server started at D:\\JupyterPrograms\\0-CHAT_GPT\\EXPERIMENTS\\ML_CHAMP\\QA_Pipeline\\QA_Pipeline_Testing\\build :\n",
      "\tLog in at http://127.0.0.1:5002\n"
     ]
    }
   ],
   "source": [
    "# server_directory='/D:/JupyterPrograms/0-CHAT_GPT/EXPERIMENTS/ML_CHAMP/QA_Pipeline/QA_Pipeline_Testing/', port=5001\n",
    "# project.start_server()\n",
    "project.start_ml_champ_server(port=5002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d43ae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for running WINDOWS server on port 5002 using command: netstat -aon | findstr \":5002 \"\n",
      "\n",
      "Found running server on pid 18696 from\n",
      "\t   TCP    0.0.0.0:5002           0.0.0.0:0              LISTENING       18696\n",
      "Shutting down server on port 5002 and pid 18696\n"
     ]
    }
   ],
   "source": [
    "# project.stop_server(5001)      # Default is 5001 (MLflow server)\n",
    "# project.stop_server(5000)  # Default ML CHAMP server port\n",
    "# project.stop_server(5002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b7587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f30d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e443e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d424a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd423f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef502e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers_env]",
   "language": "python",
   "name": "conda-env-transformers_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
