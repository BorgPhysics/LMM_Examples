{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750077b7",
   "metadata": {},
   "source": [
    "## GPT Sentence Transformer and FAISS example 3\n",
    "\n",
    "Now, we want to expand on the first example to create a vector index that can be searched on an ad-hoc basis.  Additionally, I would like to expand on creating additional vector indexes that have sub-topics that can be used for targeted searches.  For example, it might be helpful to have a set of indexes that are purely related to various topics.  Each of these indexes would have some associated text that would be queried first to determine which index(es) were most related to the question.  Then the query would be applied to the sub-indexes to find relevant documents and text.\n",
    "\n",
    "[Sentence Transformers home page](https://www.sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfbe24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hugh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "D:\\ProgramData\\Anaconda3\\envs\\transformers_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer for sentence tokenization\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from directives_processor import DirectivesProcessor\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec679003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "dp = DirectivesProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87b146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 0, File Name: AI_1.txt\n",
      "Document ID: 1, File Name: ClimateChange_1.txt\n",
      "Document ID: 2, File Name: CulturalDiversityAndTraditions_1.txt\n",
      "Document ID: 3, File Name: FinancialMarkets_1.txt\n",
      "Document ID: 4, File Name: HistoryAndHistoricalEvents_1.txt\n",
      "Document ID: 5, File Name: Terrorism_1.txt\n",
      "Document ID: 6, File Name: WorldHealthIssues_1.txt\n"
     ]
    }
   ],
   "source": [
    "# Load the documents and their id mappings\n",
    "documents_mapping = {}\n",
    "\n",
    "def load_documents_from_folder(folder_path):    \n",
    "    file_names = os.listdir(folder_path)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                document_content = file.read()\n",
    "                documents_mapping[len(documents_mapping)] = {\n",
    "                    'file_name': file_name,\n",
    "                    'content': document_content\n",
    "                }\n",
    "    \n",
    "    return documents_mapping\n",
    "\n",
    "# Load documents from the 'data' folder\n",
    "data_folder = 'data'  # Replace 'data' with your folder name\n",
    "documents_mapping = load_documents_from_folder(data_folder)\n",
    "\n",
    "# Construct documents array from the values of documents_mapping\n",
    "documents = [doc_info['content'] for doc_info in documents_mapping.values()]\n",
    "\n",
    "# Print the document ID to file name mapping\n",
    "for doc_id, doc_info in documents_mapping.items():\n",
    "    print(f\"Document ID: {doc_id}, File Name: {doc_info['file_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac84f1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC NAMES: ['AI', 'Climate Change', 'World Health Issues', 'Cultural Diversity']\n",
      "\n",
      "TOPIC TITLES:\n",
      " ['Artificial Intelligence and Machine Learning', 'Documents covering climate change, global warming, environmental impacts, renewable energy, etc.', 'Gather documents related to health topics such as diseases, medical advancements, healthcare policies, etc.', 'Gather information about different cultures, traditions, languages, and societal practices.']\n",
      "\n",
      "AI DIRECTIVES:\n",
      " Artificial Intelligence and Machine Learning\n",
      "\tGlobal Implementation of AI and ML\n",
      "\t\tIdentify key AI/ML initiatives in various countries.\n",
      "\t\tAnalyze adoption rates and trends of AI/ML technologies.\n",
      "\t\tCompare and contrast scope and scale of AI/ML applications in different nations.\n",
      "\tPolicy and Regulation Variance\n",
      "\t\tInvestigate legal frameworks and policies governing AI/ML in different countries.\n",
      "\t\tAssess ethical considerations and regulatory disparities.\n",
      "\t\tHighlight geopolitical implications of AI/ML disparities among nations.\n",
      "\tTechnological Advancements and Competitiveness\n",
      "\t\tEvaluate advancements in AI/ML research and development worldwide.\n",
      "\t\tExamine competitive edge of different nations in AI/ML innovation.\n",
      "\t\tCompare investments made by various countries in AI/ML infrastructure and education.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of 'questions' from the document directives\n",
    "directive_embeddings = {}\n",
    "topic_directives = []\n",
    "topic_names = dp.get_topic_names()\n",
    "print('TOPIC NAMES:', topic_names)\n",
    "topic_titles = dp.get_topic_titles()\n",
    "print('\\nTOPIC TITLES:\\n', topic_titles)\n",
    "print('\\nAI DIRECTIVES:\\n', dp.get_topic_text('AI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc880ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_embeddings.shape torch.Size([42, 768])\n",
      "Length of the index: 42\n",
      "sentence_embeddings.shape torch.Size([43, 768])\n",
      "Length of the index: 85\n",
      "sentence_embeddings.shape torch.Size([110, 768])\n",
      "Length of the index: 195\n",
      "sentence_embeddings.shape torch.Size([101, 768])\n",
      "Length of the index: 296\n",
      "sentence_embeddings.shape torch.Size([134, 768])\n",
      "Length of the index: 430\n",
      "sentence_embeddings.shape torch.Size([338, 768])\n",
      "Length of the index: 768\n",
      "sentence_embeddings.shape torch.Size([63, 768])\n",
      "Length of the index: 831\n",
      "Final length of the index: 831\n"
     ]
    }
   ],
   "source": [
    "# Populate the index and track the sentence ids and locations\n",
    "\n",
    "index = faiss.IndexFlatL2(768)  # Create an index\n",
    "# Maintain a mapping between sentence embeddings' index and their original sentences\n",
    "sentence_to_index_mapping = {}\n",
    "\n",
    "# Load the docs into the index\n",
    "for idx, doc in enumerate(documents):\n",
    "    sentences = nltk.sent_tokenize(doc)\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    print('sentence_embeddings.shape', sentence_embeddings.shape)   \n",
    "    \n",
    "    for sentence_idx, embedding in enumerate(sentence_embeddings):\n",
    "        # Add sentence embedding to the index\n",
    "        index.add(np.expand_dims(embedding, axis=0))\n",
    "        \n",
    "        # Track the mapping between sentence index and its embedding index\n",
    "        sentence_to_index_mapping[len(sentence_to_index_mapping)] = {\n",
    "            'document_index': idx,\n",
    "            'sentence_index': sentence_idx,\n",
    "            'sentence_text': sentences[sentence_idx]  # Save the actual sentence\n",
    "        }\n",
    "    \n",
    "    print(\"Length of the index:\", index.ntotal)\n",
    "    \n",
    "print(\"Final length of the index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c82734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: {   'document_index': 0,\n",
      "           'sentence_index': 0,\n",
      "           'sentence_text': 'Artificial intelligence (AI) vs. machine learning '\n",
      "                            '(ML)\\n'\n",
      "                            'You might hear people use artificial intelligence '\n",
      "                            '(AI) and machine learning (ML) interchangeably, '\n",
      "                            'especially when discussing big data, predictive '\n",
      "                            'analytics, and other digital transformation '\n",
      "                            'topics.'},\n",
      "    1: {   'document_index': 0,\n",
      "           'sentence_index': 1,\n",
      "           'sentence_text': 'The confusion is understandable as artificial '\n",
      "                            'intelligence and machine learning are closely '\n",
      "                            'related.'},\n",
      "    2: {   'document_index': 0,\n",
      "           'sentence_index': 2,\n",
      "           'sentence_text': 'However, these trending technologies differ in '\n",
      "                            'several ways, including scope, applications, and '\n",
      "                            'more.'},\n",
      "    3: {   'document_index': 0,\n",
      "           'sentence_index': 3,\n",
      "           'sentence_text': 'Increasingly AI and ML products have proliferated '\n",
      "                            'as businesses use them to process and analyze '\n",
      "                            'immense volumes of data, drive better '\n",
      "                            'decision-making, generate recommendations and '\n",
      "                            'insights in real time, and create accurate '\n",
      "                            'forecasts and predictions.'},\n",
      "    4: {   'document_index': 0,\n",
      "           'sentence_index': 4,\n",
      "           'sentence_text': 'So, what exactly is the difference when it comes '\n",
      "                            'to ML vs. AI, how are ML and AI connected, and '\n",
      "                            'what do these terms mean in practice for '\n",
      "                            'organizations today?'},\n",
      "    5: {   'document_index': 0,\n",
      "           'sentence_index': 5,\n",
      "           'sentence_text': 'We’ll break down AI vs. ML and explore how these '\n",
      "                            'two innovative concepts are related and what '\n",
      "                            'makes them different from each other.'},\n",
      "    6: {   'document_index': 0,\n",
      "           'sentence_index': 6,\n",
      "           'sentence_text': 'Get started for free\\n'\n",
      "                            'What is artificial intelligence?'},\n",
      "    7: {   'document_index': 0,\n",
      "           'sentence_index': 7,\n",
      "           'sentence_text': 'Artificial intelligence is a broad field, which '\n",
      "                            'refers to the use of technologies to build '\n",
      "                            'machines and computers that have the ability to '\n",
      "                            'mimic cognitive functions associated with human '\n",
      "                            'intelligence, such as being able to see, '\n",
      "                            'understand, and respond to spoken or written '\n",
      "                            'language, analyze data, make recommendations, and '\n",
      "                            'more.'},\n",
      "    8: {   'document_index': 0,\n",
      "           'sentence_index': 8,\n",
      "           'sentence_text': 'Although artificial intelligence is often thought '\n",
      "                            'of as a system in itself, it is a set of '\n",
      "                            'technologies implemented in a system to enable it '\n",
      "                            'to reason, learn, and act to solve a complex '\n",
      "                            'problem.'},\n",
      "    9: {   'document_index': 0,\n",
      "           'sentence_index': 9,\n",
      "           'sentence_text': 'What is machine learning?'}}\n"
     ]
    }
   ],
   "source": [
    "# Check the first 10 entries\n",
    "# Slice the first 10 items of the dictionary\n",
    "first_10_items = {k: sentence_to_index_mapping[k] for k in sorted(sentence_to_index_mapping)[:10]}\n",
    "\n",
    "# Pretty print the first 10 items\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(first_10_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bef3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER QUERY: How is global health affected by climate change?\n",
      "Most similar prompts to the user query:\n",
      "\n",
      "Document Index: 6\t Sentence Index: 22\t Filename: WorldHealthIssues_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tWe are now at a point where climate change is clearly with us, and much more attention needs to be put on minimizing the impacts on global health through adaptation or enhancing resilience.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 6\t Sentence Index: 19\t Filename: WorldHealthIssues_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tAs we know from the pandemic, preparedness is key, and we are far from prepared for the health impacts of a warmer climate.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 6\t Sentence Index: 16\t Filename: WorldHealthIssues_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tImpact of climate changeA child looks out over a dried up lake\n",
      "“Climate change is already affecting the health of millions of people all over the world, and more importantly, climate change will worsen throughout this century.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 1\t Sentence Index: 20\t Filename: ClimateChange_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tPeople are experiencing climate change in diverse ways\n",
      "Climate change can affect our health, ability to grow food, housing, safety and work.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 1\t Sentence Index: 21\t Filename: ClimateChange_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tSome of us are already more vulnerable to climate impacts, such as people living in small island nations and other developing countries.\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a user query against the index and get the top 5\n",
    "# User query or question\n",
    "user_query = \"How does AI affect global health?\"\n",
    "user_query = \"How is global health affected by climate change?\"\n",
    "\n",
    "# Encode user query into an embedding\n",
    "user_query_embedding = model.encode(user_query, convert_to_tensor=True).numpy()\n",
    "\n",
    "# Search in FAISS index\n",
    "k = 5  # Number of most similar prompts to retrieve\n",
    "D, I = index.search(np.array([user_query_embedding]), k)\n",
    "\n",
    "# Retrieve sentences and corresponding documents based on valid indices\n",
    "most_similar_prompts = []\n",
    "for idx in I[0]:\n",
    "    if idx in sentence_to_index_mapping:\n",
    "        mapping = sentence_to_index_mapping[idx]\n",
    "        doc_idx = mapping['document_index']\n",
    "        sentence_idx = mapping['sentence_index']\n",
    "        sentence = mapping['sentence_text']\n",
    "        most_similar_prompts.append((doc_idx, sentence_idx, sentence))\n",
    "\n",
    "print('USER QUERY:', user_query)\n",
    "print(\"Most similar prompts to the user query:\")\n",
    "for doc_idx, sentence_idx, sentence in most_similar_prompts:\n",
    "    filename = documents_mapping.get(doc_idx, '').get('file_name')\n",
    "    print(f\"\\nDocument Index: {doc_idx}\\t Sentence Index: {sentence_idx}\\t Filename: {filename}\")\n",
    "    print(f\"Sentence:\\n******************************\\n\\t{sentence}\")\n",
    "    print('******************************\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b9093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89630aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5704cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ee207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers_env]",
   "language": "python",
   "name": "conda-env-transformers_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
