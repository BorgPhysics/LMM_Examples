{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750077b7",
   "metadata": {},
   "source": [
    "## GPT Sentence Transformer and FAISS example 4\n",
    "\n",
    "Now, we want to expand on the first example to create a vector index that can be searched on an ad-hoc basis.  Additionally, I would like to expand on creating additional vector indexes that have sub-topics that can be used for targeted searches.  For example, it might be helpful to have a set of indexes that are purely related to various topics.  Each of these indexes would have some associated text that would be queried first to determine which index(es) were most related to the question.  Then the query would be applied to the sub-indexes to find relevant documents and text.\n",
    "\n",
    "[Sentence Transformers home page](https://www.sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdfbe24a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hugh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer for sentence tokenization\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from directives_processor import DirectivesProcessor\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec679003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "dp = DirectivesProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca29733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BART tokenizer\n",
    "summarization_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Load the pre-trained BART model for summarization\n",
    "summarization_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0291e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Load the pre-trained BERT model for question answering\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87b146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 0, File Name: AI_1.txt\n",
      "Document ID: 1, File Name: ClimateChange_1.txt\n",
      "Document ID: 2, File Name: CulturalDiversityAndTraditions_1.txt\n",
      "Document ID: 3, File Name: FinancialMarkets_1.txt\n",
      "Document ID: 4, File Name: HistoryAndHistoricalEvents_1.txt\n",
      "Document ID: 5, File Name: Terrorism_1.txt\n",
      "Document ID: 6, File Name: WorldHealthIssues_1.txt\n"
     ]
    }
   ],
   "source": [
    "# Load the documents and their id mappings\n",
    "documents_mapping = {}\n",
    "\n",
    "def load_documents_from_folder(folder_path):    \n",
    "    file_names = os.listdir(folder_path)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                document_content = file.read()\n",
    "                documents_mapping[len(documents_mapping)] = {\n",
    "                    'file_name': file_name,\n",
    "                    'content': document_content\n",
    "                }\n",
    "    \n",
    "    return documents_mapping\n",
    "\n",
    "# Load documents from the 'data' folder\n",
    "data_folder = 'data'  # Replace 'data' with your folder name\n",
    "documents_mapping = load_documents_from_folder(data_folder)\n",
    "\n",
    "# Construct documents array from the values of documents_mapping\n",
    "documents = [doc_info['content'] for doc_info in documents_mapping.values()]\n",
    "\n",
    "# Print the document ID to file name mapping\n",
    "for doc_id, doc_info in documents_mapping.items():\n",
    "    print(f\"Document ID: {doc_id}, File Name: {doc_info['file_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a31d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Is Climate Change?\n",
      "Climate change refers to long-term shifts in temperatures and weather patter\n"
     ]
    }
   ],
   "source": [
    "# Get the document by doc id:\n",
    "doc_info = documents_mapping.get(1)\n",
    "print(doc_info['content'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e871360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 0, File Name: AI_1.txt\n",
      "******************************\n",
      "Summary:\n",
      "\t Artificial intelligence (AI) and machine learning (ML) are closely related. However, these technologies differ in several ways, including scope, applications, and more. Here, we explore how these two innovative concepts are related and what makes them different from each other.\n",
      "******************************\n",
      "\n",
      "Document ID: 1, File Name: ClimateChange_1.txt\n",
      "******************************\n",
      "Summary:\n",
      "\t The average temperature of the Earth’s surface is 1.1°C warmer than it was in the late 1800s (before the industrial revolution) The last decade (2011-2020) was the warmest on record, and each of the last four decades has been warmer than any previous decade since 1850.\n",
      "******************************\n",
      "\n",
      "Document ID: 2, File Name: CulturalDiversityAndTraditions_1.txt\n",
      "******************************\n",
      "Summary:\n",
      "\t Cultural diversity is the quality of diverse or different cultures, as opposed to monoculture. It has a variety of meanings in different contexts, sometimes applying to cultural products like art works in museums or entertainment available online. Since the 1990s, UNESCO has mainly used \"cultural diversity\" for the international aspects of diversity, preferring the term \"cultural pluralism\" for diversity within a country.\n",
      "******************************\n",
      "\n",
      "Document ID: 3, File Name: FinancialMarkets_1.txt\n",
      "******************************\n",
      "Summary:\n",
      "\t Financial markets are vital to the smooth operation of capitalist economies. Financial markets create securities products that provide a return for those with excess funds (investors/lenders) Financial markets rely heavily on informational transparency to ensure that the markets set prices that are efficient and appropriate.\n",
      "******************************\n",
      "\n",
      "Document ID: 4, File Name: HistoryAndHistoricalEvents_1.txt\n",
      "******************************\n",
      "Summary:\n",
      "\t The Sept. 11 terror attacks were such a unifying event for modern Americans. More than seven-in-ten Republicans and Democrats name the attacks as one of their top 10 historic events. For Millennials and Gen Xers, the 9/11 terror attacks and the Obama election leads the list by a greater margin than for other generations.\n",
      "******************************\n",
      "\n",
      "Document ID: 5, File Name: Terrorism_1.txt\n",
      "******************************\n",
      "Summary:\n",
      "\t Terrorism is the use of intentional violence and fear to achieve political or ideological aims. The terms \"terrorist\" and \"terrorism\" originated during the French Revolution of the late 18th century. Varied political organizations have been accused of using terrorism to achieve their objectives. There is no consensus as to whether terrorism should be regarded as a war crime.\n",
      "******************************\n",
      "\n",
      "Document ID: 6, File Name: WorldHealthIssues_1.txt\n",
      "******************************\n",
      "Summary:\n",
      "\t IHME faculty members and research scientists share their insights on the most critical health issues to watch in 2023. Most of our experts pointed to issues that were impacted in some way by the pandemic, like long COVID and mental health. They also offered potential interventions to address the threats.\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=150):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the summary tokens into text\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# # Summarize one document\n",
    "# # Example usage to summarize a document\n",
    "# document_text = documents[0]\n",
    "# summary = summarize_text(document_text, summarization_model, summarization_tokenizer)\n",
    "# print(\"Summary:\", summary)\n",
    "\n",
    "# Summarize each document\n",
    "for doc_id, doc_info in documents_mapping.items():\n",
    "    print(f\"Document ID: {doc_id}, File Name: {doc_info['file_name']}\")\n",
    "    print('******************************')\n",
    "    document_text = doc_info['content']\n",
    "    summary = summarize_text(document_text, summarization_model, summarization_tokenizer)\n",
    "    print('Summary:\\n\\t', summary)\n",
    "    print('******************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ac84f1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC NAMES: ['AI', 'Climate Change', 'World Health Issues', 'Cultural Diversity']\n",
      "\n",
      "TOPIC TITLES:\n",
      " ['Artificial Intelligence and Machine Learning', 'Documents covering climate change, global warming, environmental impacts, renewable energy, etc.', 'Gather documents related to health topics such as diseases, medical advancements, healthcare policies, etc.', 'Gather information about different cultures, traditions, languages, and societal practices.']\n",
      "\n",
      "AI DIRECTIVES:\n",
      " Artificial Intelligence and Machine Learning\n",
      "\tGlobal Implementation of AI and ML\n",
      "\t\tIdentify key AI/ML initiatives in various countries.\n",
      "\t\tAnalyze adoption rates and trends of AI/ML technologies.\n",
      "\t\tCompare and contrast scope and scale of AI/ML applications in different nations.\n",
      "\tPolicy and Regulation Variance\n",
      "\t\tInvestigate legal frameworks and policies governing AI/ML in different countries.\n",
      "\t\tAssess ethical considerations and regulatory disparities.\n",
      "\t\tHighlight geopolitical implications of AI/ML disparities among nations.\n",
      "\tTechnological Advancements and Competitiveness\n",
      "\t\tEvaluate advancements in AI/ML research and development worldwide.\n",
      "\t\tExamine competitive edge of different nations in AI/ML innovation.\n",
      "\t\tCompare investments made by various countries in AI/ML infrastructure and education.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of 'questions' from the document directives\n",
    "directive_embeddings = {}\n",
    "topic_directives = []\n",
    "topic_names = dp.get_topic_names()\n",
    "print('TOPIC NAMES:', topic_names)\n",
    "topic_titles = dp.get_topic_titles()\n",
    "print('\\nTOPIC TITLES:\\n', topic_titles)\n",
    "print('\\nAI DIRECTIVES:\\n', dp.get_topic_text('AI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cc880ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index from doc_index.index\n",
      "Loaded sentence_to_index_mapping from sentence_to_index_mapping.json\n",
      "Final length of the index: 831\n"
     ]
    }
   ],
   "source": [
    "index_name = 'doc_index.index'\n",
    "mapping_name = 'sentence_to_index_mapping.json'\n",
    "index = None\n",
    "sentence_to_index_mapping = None\n",
    "\n",
    "if os.path.exists(index_name) and os.path.exists(mapping_name):\n",
    "    index = faiss.read_index(index_name)\n",
    "    print(f\"Loaded index from {index_name}\")\n",
    "\n",
    "    with open(mapping_name, 'r') as mapping_file:\n",
    "        sentence_to_index_mapping = json.load(mapping_file)\n",
    "    print(f\"Loaded sentence_to_index_mapping from {mapping_name}\")\n",
    "else:\n",
    "    # Populate the index and track the sentence ids and locations\n",
    "    index = faiss.IndexFlatL2(768)  # Create an index\n",
    "    # Maintain a mapping between sentence embeddings' index and their original sentences\n",
    "    sentence_to_index_mapping = {}\n",
    "\n",
    "    # Load the docs into the index\n",
    "    for idx, doc in enumerate(documents):\n",
    "        sentences = nltk.sent_tokenize(doc)\n",
    "        sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        print('sentence_embeddings.shape', sentence_embeddings.shape)   \n",
    "\n",
    "        for sentence_idx, embedding in enumerate(sentence_embeddings):\n",
    "            # Add sentence embedding to the index\n",
    "            index.add(np.expand_dims(embedding, axis=0))\n",
    "\n",
    "            # Track the mapping between sentence index and its embedding index\n",
    "            sentence_to_index_mapping[len(sentence_to_index_mapping)] = {\n",
    "                'document_index': idx,\n",
    "                'sentence_index': sentence_idx,\n",
    "                'sentence_text': sentences[sentence_idx]  # Save the actual sentence\n",
    "            }\n",
    "\n",
    "        print(\"Length of the index:\", index.ntotal)\n",
    "            \n",
    "    # Save the index and mapping\n",
    "    faiss.write_index(index, index_name)\n",
    "    with open(mapping_name, 'w') as mapping_file:\n",
    "        json.dump(sentence_to_index_mapping, mapping_file)\n",
    "    \n",
    "print(\"Final length of the index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f5ab3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: None \tReloading index.\n",
      "index: <faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x00000198DF943780> >\n"
     ]
    }
   ],
   "source": [
    "# Test the saving and loading of an index\n",
    "# index_name = 'doc_index.index'\n",
    "# faiss.write_index(index, index_name)\n",
    "\n",
    "# index = None\n",
    "# print('index:', index, '\\tReloading index.')\n",
    "# index = faiss.read_index(index_name)\n",
    "# print('index:', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59c82734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '0': {   'document_index': '2d28937a-69ae-4fc4-8543-1b9b5498fab2',\n",
      "             'sentence_index': 0,\n",
      "             'sentence_text': 'Artificial intelligence (AI) vs. machine '\n",
      "                              'learning (ML)\\n'\n",
      "                              'You might hear people use artificial '\n",
      "                              'intelligence (AI) and machine learning (ML) '\n",
      "                              'interchangeably, especially when discussing big '\n",
      "                              'data, predictive analytics, and other digital '\n",
      "                              'transformation topics.'},\n",
      "    '1': {   'document_index': '2d28937a-69ae-4fc4-8543-1b9b5498fab2',\n",
      "             'sentence_index': 1,\n",
      "             'sentence_text': 'The confusion is understandable as artificial '\n",
      "                              'intelligence and machine learning are closely '\n",
      "                              'related.'},\n",
      "    '10': {   'document_index': '2d28937a-69ae-4fc4-8543-1b9b5498fab2',\n",
      "              'sentence_index': 10,\n",
      "              'sentence_text': 'Machine learning is a subset of artificial '\n",
      "                               'intelligence that automatically enables a '\n",
      "                               'machine or system to learn and improve from '\n",
      "                               'experience.'},\n",
      "    '100': {   'document_index': 'a97167de-a09a-4d43-92a2-efa489a237d7',\n",
      "               'sentence_index': 15,\n",
      "               'sentence_text': 'It emphasises an ongoing process of '\n",
      "                                'interaction and dialogue between cultures.'},\n",
      "    '101': {   'document_index': 'a97167de-a09a-4d43-92a2-efa489a237d7',\n",
      "               'sentence_index': 16,\n",
      "               'sentence_text': '[4][5] This meaning has been promoted to the '\n",
      "                                'international community by UNESCO, since the '\n",
      "                                '2001 Universal Declaration on Cultural '\n",
      "                                'Diversity.'}}\n"
     ]
    }
   ],
   "source": [
    "# Check the first 10 entries\n",
    "# Slice the first 10 items of the dictionary\n",
    "first_5_items = {k: sentence_to_index_mapping[k] for k in sorted(sentence_to_index_mapping)[:5]}\n",
    "\n",
    "# Pretty print the first 5 items\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(first_5_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8bef3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER QUERY: How is global health affected by climate change?\n",
      "Most similar prompts to the user query:\n",
      "\n",
      "Document Index: 6\t Sentence Index: 22\t Filename: WorldHealthIssues_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tWe are now at a point where climate change is clearly with us, and much more attention needs to be put on minimizing the impacts on global health through adaptation or enhancing resilience.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 6\t Sentence Index: 19\t Filename: WorldHealthIssues_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tAs we know from the pandemic, preparedness is key, and we are far from prepared for the health impacts of a warmer climate.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 6\t Sentence Index: 16\t Filename: WorldHealthIssues_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tImpact of climate changeA child looks out over a dried up lake\n",
      "“Climate change is already affecting the health of millions of people all over the world, and more importantly, climate change will worsen throughout this century.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 1\t Sentence Index: 20\t Filename: ClimateChange_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tPeople are experiencing climate change in diverse ways\n",
      "Climate change can affect our health, ability to grow food, housing, safety and work.\n",
      "******************************\n",
      "\n",
      "\n",
      "Document Index: 1\t Sentence Index: 21\t Filename: ClimateChange_1.txt\n",
      "Sentence:\n",
      "******************************\n",
      "\tSome of us are already more vulnerable to climate impacts, such as people living in small island nations and other developing countries.\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a user query against the index and get the top 5\n",
    "# User query or question\n",
    "user_query = \"How does AI affect global health?\"\n",
    "user_query = \"How is global health affected by climate change?\"\n",
    "\n",
    "# Encode user query into an embedding\n",
    "user_query_embedding = model.encode(user_query, convert_to_tensor=True).numpy()\n",
    "\n",
    "# Search in FAISS index\n",
    "k = 5  # Number of most similar prompts to retrieve\n",
    "D, I = index.search(np.array([user_query_embedding]), k)\n",
    "\n",
    "# Retrieve sentences and corresponding documents based on valid indices\n",
    "most_similar_prompts = []\n",
    "for idx in I[0]:\n",
    "    if idx in sentence_to_index_mapping:\n",
    "        mapping = sentence_to_index_mapping[idx]\n",
    "        doc_idx = mapping['document_index']\n",
    "        sentence_idx = mapping['sentence_index']\n",
    "        sentence = mapping['sentence_text']\n",
    "        most_similar_prompts.append((doc_idx, sentence_idx, sentence))\n",
    "\n",
    "print('USER QUERY:', user_query)\n",
    "print(\"Most similar prompts to the user query:\")\n",
    "for doc_idx, sentence_idx, sentence in most_similar_prompts:\n",
    "    filename = documents_mapping.get(doc_idx, '').get('file_name')\n",
    "    print(f\"\\nDocument Index: {doc_idx}\\t Sentence Index: {sentence_idx}\\t Filename: {filename}\")\n",
    "    print(f\"Sentence:\\n******************************\\n\\t{sentence}\")\n",
    "    print('******************************\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "611fc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context, model, tokenizer):\n",
    "    # Tokenize the input question and context text\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Get the model's prediction for the start and end positions\n",
    "    with torch.no_grad():  # Ensure no gradients are computed\n",
    "        outputs = model(**inputs)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "\n",
    "    # Convert start and end scores to numpy arrays for further processing\n",
    "    start_scores = start_scores.detach().cpu().numpy()\n",
    "    end_scores = end_scores.detach().cpu().numpy()\n",
    "\n",
    "    # Find the tokens with the highest start and end scores\n",
    "    start_idx = np.argmax(start_scores)\n",
    "    end_idx = np.argmax(end_scores)\n",
    "\n",
    "    # Decode the tokens into the answer text\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1]))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f31c5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_large_document(user_question, faiss_sentences, large_document, qa_model, tokenizer):\n",
    "    # Concatenate FAISS sentences with the user question to form the context\n",
    "    faiss_context = ' '.join(faiss_sentences)\n",
    "    context = user_question + \" \" + faiss_context\n",
    "\n",
    "    # Tokenize the context and large document separately\n",
    "    tokenized_context = tokenizer.encode_plus(context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    tokenized_document = tokenizer.encode_plus(large_document, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Trim or truncate the tokenized document to fit within the remaining space\n",
    "    max_seq_len = tokenizer.model_max_length - tokenized_context['input_ids'].size(1) - 3  # Account for special tokens [CLS], [SEP], etc.\n",
    "    input_ids_doc = tokenized_document['input_ids'][:, :max_seq_len]\n",
    "    attention_mask_doc = tokenized_document['attention_mask'][:, :max_seq_len]\n",
    "\n",
    "    # Concatenate the tokenized context and truncated document\n",
    "    input_ids = torch.cat([tokenized_context['input_ids'], input_ids_doc], dim=1)\n",
    "    attention_mask = torch.cat([tokenized_context['attention_mask'], attention_mask_doc], dim=1)\n",
    "\n",
    "    # Answer the question based on the combined context and document\n",
    "    with torch.no_grad():\n",
    "        outputs = qa_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Process outputs to get the answer\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores)\n",
    "\n",
    "    # Decode the tokens into the answer text\n",
    "    answer = tokenizer.decode(input_ids[0][start_idx:end_idx+1], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e8538d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Is Climate Change?\n",
      "Climate change refers to long-term shifts in temperatures and weather patterns. Such shifts can be natural, due to changes in \n",
      "\n",
      "User  query:\n",
      "\t How is global health affected by climate change?\n",
      "Answer:\n",
      "\t climate change is already affecting the health of millions of people all over the world\n"
     ]
    }
   ],
   "source": [
    "# most_similar_prompts\n",
    "# Get the document by doc id:\n",
    "doc_info = documents_mapping.get(1)\n",
    "document = doc_info['content']\n",
    "print(document[:150])\n",
    "\n",
    "# Extract only the sentences from most_similar_prompts\n",
    "faiss_sentences = [sentence for _, _, sentence in most_similar_prompts]\n",
    "\n",
    "print('\\nUser  query:\\n\\t', user_query)\n",
    "answer = process_single_large_document(user_query, faiss_sentences, document, qa_model, qa_tokenizer)\n",
    "print(\"Answer:\\n\\t\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6313d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94431a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5704cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ee207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a392d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f95a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191a0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5cea8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179df579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers_env]",
   "language": "python",
   "name": "conda-env-transformers_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
