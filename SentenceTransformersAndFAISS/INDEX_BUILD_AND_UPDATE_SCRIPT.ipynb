{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfdfe32",
   "metadata": {},
   "source": [
    "## Index Build and Update Script\n",
    "\n",
    "Whoops!  I only had the initial build script but nothing to actually update it with new documents.  This notebook will take care of that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae5116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hugh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "D:\\ProgramData\\Anaconda3\\envs\\transformers_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer for sentence tokenization\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import uuid\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e412296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##########  VARIABLES  ##########\n",
    "#################################\n",
    "\n",
    "index_transformer_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "index_transformer_model = SentenceTransformer(index_transformer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d302a",
   "metadata": {},
   "source": [
    "### Load the Existing Document Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351826b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file already exists at: D:\\JupyterPrograms\\0-CHAT_GPT\\EXPERIMENTS\\Sentence Transformers and FAISS\\document_guid_lookup.csv\n",
      "Loaded existing DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_guid</th>\n",
       "      <th>doc_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_guid, doc_filename]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir = os.getcwd()\n",
    "docs_dir = os.path.join(data_dir, 'data')\n",
    "document_guid_lookup_csv = os.path.join(data_dir, 'document_guid_lookup.csv')\n",
    "document_guid_lookup_df = None\n",
    "\n",
    "def get_document_guid_lookup():\n",
    "    global document_guid_lookup_df\n",
    "    # See if the DataFrame already exists.  Load it or create it if it doesn't\n",
    "    \n",
    "    if document_guid_lookup_df is None:\n",
    "        # Check if the CSV file exists\n",
    "        if not os.path.exists(document_guid_lookup_csv):\n",
    "            # If the file doesn't exist, create a new DataFrame\n",
    "            # Create an empty DataFrame with column names\n",
    "            column_names = ['doc_guid', 'doc_filename']\n",
    "            document_guid_lookup_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "            # Save the DataFrame to the CSV file\n",
    "            document_guid_lookup_df.to_csv(document_guid_lookup_csv, index=False)\n",
    "            print(f\"CSV file created at: {document_guid_lookup_csv}\")\n",
    "        else:\n",
    "            # If the file already exists, load it into a DataFrame\n",
    "            document_guid_lookup_df = pd.read_csv(document_guid_lookup_csv)\n",
    "            print(f\"CSV file already exists at: {document_guid_lookup_csv}\")\n",
    "            print(\"Loaded existing DataFrame:\")\n",
    "            \n",
    "    return document_guid_lookup_df\n",
    "\n",
    "document_guid_lookup_df = get_document_guid_lookup()\n",
    "document_guid_lookup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12dab8",
   "metadata": {},
   "source": [
    "### Load an existing index or build from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51da7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded index from sentences.index\n",
      "Loaded sentence_to_index_mapping from sentence_to_index_mapping.json\n",
      "Final length of the index: 0\n",
      "sentence_to_index_mapping {}\n"
     ]
    }
   ],
   "source": [
    "index_name = 'sentences.index'\n",
    "mapping_name = 'sentence_to_index_mapping.json'\n",
    "index = None\n",
    "sentence_to_index_mapping = None\n",
    "\n",
    "def add_document_to_index(guid, file_name):   \n",
    "    global index\n",
    "    global sentence_to_index_mapping\n",
    "    global index_transformer_model\n",
    "    \n",
    "    file_path = os.path.join(docs_dir, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            document = file.read()\n",
    "\n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        sentence_embeddings = index_transformer_model.encode(sentences, convert_to_tensor=True)                  \n",
    "\n",
    "        for sentence_idx, embedding in enumerate(sentence_embeddings):\n",
    "            # Add sentence embedding to the index\n",
    "            index.add(np.expand_dims(embedding, axis=0))\n",
    "\n",
    "            # Track the mapping between sentence index and its embedding index\n",
    "            sentence_to_index_mapping[len(sentence_to_index_mapping)] = {\n",
    "                'document_index': guid,\n",
    "                'sentence_index': sentence_idx,\n",
    "                'sentence_text': sentences[sentence_idx]  # Save the actual sentence\n",
    "            }\n",
    "\n",
    "        print(\"Length of the index:\", index.ntotal)\n",
    "    \n",
    "\n",
    "def load_existing_index():    \n",
    "    global index\n",
    "    global sentence_to_index_mapping\n",
    "    if os.path.exists(index_name) and os.path.exists(mapping_name):\n",
    "        index = faiss.read_index(index_name)\n",
    "        print(f\"Loaded index from {index_name}\")\n",
    "\n",
    "        with open(mapping_name, 'r') as mapping_file:\n",
    "            sentence_to_index_mapping = json.load(mapping_file)\n",
    "        print(f\"Loaded sentence_to_index_mapping from {mapping_name}\")\n",
    "    else:\n",
    "        print('Index does not exist.  Attempting to build it.')\n",
    "        # Populate the index and track the sentence ids and locations\n",
    "        index = faiss.IndexFlatL2(768)  # Create an index\n",
    "        # Maintain a mapping between sentence embeddings' index and their original sentences\n",
    "        sentence_to_index_mapping = {}\n",
    "        \n",
    "        # Load the docs into the index\n",
    "        for _, row in document_guid_lookup_df.iterrows():\n",
    "            guid = row['doc_guid']\n",
    "            file_name = row['doc_filename']\n",
    "            add_document_to_index(guid, file_name)\n",
    "            \n",
    "        # Save the index and mapping\n",
    "        faiss.write_index(index, index_name)\n",
    "        with open(mapping_name, 'w') as mapping_file:\n",
    "            json.dump(sentence_to_index_mapping, mapping_file)\n",
    "\n",
    "    \n",
    "load_existing_index()\n",
    "print(\"Final length of the index:\", index.ntotal)\n",
    "print('sentence_to_index_mapping', sentence_to_index_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a277bef",
   "metadata": {},
   "source": [
    "### Update an existing index\n",
    "\n",
    "Updating the index involves checking for new documents, updating the document_guid_lookup_df, and loading their sentences into the index and the sentence_to_index_mapping.  Finally, save all three files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89eebf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are new entries: [{'doc_guid': 'f6126e61-5598-4393-82a3-9f2e47e78d27', 'doc_filename': 'AI_1.txt'}, {'doc_guid': 'dae8f392-ab50-4f9f-9388-19958fe90a5f', 'doc_filename': 'AI_2.txt'}, {'doc_guid': '921de1d3-0f4e-41ca-a294-bef7ed930d3d', 'doc_filename': 'AI_3.txt'}, {'doc_guid': '2b7f293c-6b45-447d-9978-eb90f67a8501', 'doc_filename': 'AI_4.txt'}, {'doc_guid': '1e7cb242-9c00-40fa-b999-dda1f084514a', 'doc_filename': 'AI_5.txt'}, {'doc_guid': 'c7674a32-1077-4b17-b766-46c9a8bce70e', 'doc_filename': 'AI_6.txt'}, {'doc_guid': 'e4239869-6536-4771-bc3b-b0ec3ab567ce', 'doc_filename': 'ClimateChange_1.txt'}, {'doc_guid': '13b52eb1-6c72-4faa-977e-dadc05c7b598', 'doc_filename': 'CulturalDiversityAndTraditions_1.txt'}, {'doc_guid': 'fc9b5a49-ed22-449c-9984-61b243c46646', 'doc_filename': 'FinancialMarkets_1.txt'}, {'doc_guid': '2f9e345e-a413-4bcf-bb90-2f717d9850a6', 'doc_filename': 'HistoryAndHistoricalEvents_1.txt'}, {'doc_guid': '6ec3579c-3800-41a4-9820-2df1b0cd2726', 'doc_filename': 'Terrorism_1.txt'}, {'doc_guid': 'e3acfa6a-79c0-454d-b5b4-62fd17fecf0e', 'doc_filename': 'WorldHealthIssues_1.txt'}]\n",
      "Updating f6126e61-5598-4393-82a3-9f2e47e78d27 AI_1.txt\n",
      "Length of the index: 42\n",
      "Updating dae8f392-ab50-4f9f-9388-19958fe90a5f AI_2.txt\n",
      "Length of the index: 1167\n",
      "Updating 921de1d3-0f4e-41ca-a294-bef7ed930d3d AI_3.txt\n",
      "Length of the index: 1238\n",
      "Updating 2b7f293c-6b45-447d-9978-eb90f67a8501 AI_4.txt\n",
      "Length of the index: 1529\n",
      "Updating 1e7cb242-9c00-40fa-b999-dda1f084514a AI_5.txt\n",
      "Length of the index: 1594\n",
      "Updating c7674a32-1077-4b17-b766-46c9a8bce70e AI_6.txt\n",
      "Length of the index: 1648\n",
      "Updating e4239869-6536-4771-bc3b-b0ec3ab567ce ClimateChange_1.txt\n",
      "Length of the index: 1691\n",
      "Updating 13b52eb1-6c72-4faa-977e-dadc05c7b598 CulturalDiversityAndTraditions_1.txt\n",
      "Length of the index: 1801\n",
      "Updating fc9b5a49-ed22-449c-9984-61b243c46646 FinancialMarkets_1.txt\n",
      "Length of the index: 1902\n",
      "Updating 2f9e345e-a413-4bcf-bb90-2f717d9850a6 HistoryAndHistoricalEvents_1.txt\n",
      "Length of the index: 2036\n",
      "Updating 6ec3579c-3800-41a4-9820-2df1b0cd2726 Terrorism_1.txt\n",
      "Length of the index: 2374\n",
      "Updating e3acfa6a-79c0-454d-b5b4-62fd17fecf0e WorldHealthIssues_1.txt\n",
      "Length of the index: 2437\n",
      "Documents checked and DataFrame updated successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def update_index_and_mappings(new_entries):   \n",
    "    global index\n",
    "    global sentence_to_index_mapping\n",
    "    \n",
    "    for record in new_entries:\n",
    "        guid = record['doc_guid']\n",
    "        file_name = record['doc_filename']\n",
    "        print('Updating', guid, file_name)\n",
    "        add_document_to_index(guid, file_name)\n",
    "            \n",
    "    # Save the index and mapping\n",
    "    faiss.write_index(index, index_name)\n",
    "    with open(mapping_name, 'w') as mapping_file:\n",
    "        json.dump(sentence_to_index_mapping, mapping_file)\n",
    "\n",
    "# TODO: This should cause an update of the FAISS index and sentence_to_index_mappings file\n",
    "def update_document_guid_lookup_df():  \n",
    "    global document_guid_lookup_df  \n",
    "    if document_guid_lookup_df is None:\n",
    "        # Load the existing one or create a new one\n",
    "        get_document_guid_lookup()\n",
    "    \n",
    "    # Get the list of files in the documents directory\n",
    "    existing_files = set(document_guid_lookup_df['doc_filename'].tolist())\n",
    "    files_to_process = [file for file in os.listdir(docs_dir) if os.path.isfile(os.path.join(docs_dir, file))]\n",
    "    \n",
    "    # Check for files that are not present in the DataFrame and add them\n",
    "    new_entries = []\n",
    "    for file_name in files_to_process:\n",
    "        if file_name not in existing_files:\n",
    "            # Generate a unique ID for the new document\n",
    "            unique_id = str(uuid.uuid4())\n",
    "\n",
    "            # Prepare a new entry for the DataFrame\n",
    "            new_entry = {'doc_guid': unique_id, 'doc_filename': file_name}\n",
    "            new_entries.append(new_entry)\n",
    "\n",
    "    # Concatenate new entries to the existing DataFrame\n",
    "    if new_entries:\n",
    "        print('There are new entries:', new_entries)\n",
    "        new_df = pd.DataFrame(new_entries)\n",
    "        document_guid_lookup_df = pd.concat([document_guid_lookup_df, new_df], ignore_index=True)\n",
    "\n",
    "        # Save the updated DataFrame to the CSV file\n",
    "#         document_guid_lookup_df.to_csv(document_guid_lookup_csv, index=False)\n",
    "        update_index_and_mappings(new_entries)\n",
    "        print(\"Documents checked and DataFrame updated successfully.\")\n",
    "    else:\n",
    "        print('No new documents were found.  No DataFrame update required.')\n",
    "#     return new_entries\n",
    "\n",
    "update_document_guid_lookup_df()\n",
    "# new_entries = update_document_guid_lookup_df()\n",
    "# new_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06ff69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677b791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c89c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f39ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe8d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers_env]",
   "language": "python",
   "name": "conda-env-transformers_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
